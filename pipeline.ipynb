{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "51611760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\admin\\.conda\\envs\\cu\\lib\\site-packages (4.49.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\.conda\\envs\\cu\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\admin\\.conda\\envs\\cu\\lib\\site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\admin\\.conda\\envs\\cu\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\.conda\\envs\\cu\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\admin\\.conda\\envs\\cu\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\admin\\.conda\\envs\\cu\\lib\\site-packages (from transformers) (2024.4.28)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\.conda\\envs\\cu\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\admin\\.conda\\envs\\cu\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\admin\\.conda\\envs\\cu\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\admin\\.conda\\envs\\cu\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\admin\\.conda\\envs\\cu\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\admin\\.conda\\envs\\cu\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\.conda\\envs\\cu\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\.conda\\envs\\cu\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\.conda\\envs\\cu\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\.conda\\envs\\cu\\lib\\site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\.conda\\envs\\cu\\lib\\site-packages (from requests->transformers) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14cfd879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import soundfile as sf\n",
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "from transformers import (\n",
    "    Wav2Vec2Processor,\n",
    "    Wav2Vec2ForCTC,\n",
    "    MarianTokenizer,\n",
    "    MarianMTModel\n",
    ")\n",
    "from TTS.api import TTS\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cfdee8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Configuration ==========\n",
    "MODEL_DIR = \"./models\"\n",
    "ASR_DIR = os.path.join(MODEL_DIR, \"asr\")\n",
    "MT_DIR = os.path.join(MODEL_DIR, \"mt\")\n",
    "TTS_CACHE = MODEL_DIR  # both TTS + vocoder model folders go here\n",
    "\n",
    "AUDIO_FILENAME = \"recorded.wav\"\n",
    "OUTPUT_WAV = \"output_jp.wav\"\n",
    "DURATION = 5  # seconds\n",
    "SAMPLE_RATE = 16000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e65e8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loading ASR model from disk.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Wav2Vec2ForCTC(\n",
       "  (wav2vec2): Wav2Vec2Model(\n",
       "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Wav2Vec2GroupNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "          (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (1-4): 4 x Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): Wav2Vec2FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): Wav2Vec2Encoder(\n",
       "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "        (conv): ParametrizedConv1d(\n",
       "          1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (padding): Wav2Vec2SamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x Wav2Vec2EncoderLayer(\n",
       "          (attention): Wav2Vec2SdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (lm_head): Linear(in_features=1024, out_features=32, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ========== ASR: Load or Save ==========\n",
    "if not os.path.exists(ASR_DIR):\n",
    "    print(\"ðŸ”½ Downloading ASR model...\")\n",
    "    processor_asr = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "    model_asr = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "    processor_asr.save_pretrained(ASR_DIR)\n",
    "    model_asr.save_pretrained(ASR_DIR)\n",
    "else:\n",
    "    print(\"âœ… Loading ASR model from disk.\")\n",
    "    processor_asr = Wav2Vec2Processor.from_pretrained(ASR_DIR)\n",
    "    model_asr = Wav2Vec2ForCTC.from_pretrained(ASR_DIR)\n",
    "model_asr.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42cc6870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loading MT model from disk.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\.conda\\envs\\cu\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "# ========== MT: Load or Save ==========\n",
    "if not os.path.exists(MT_DIR):\n",
    "    print(\"ðŸ”½ Downloading MT model...\")\n",
    "    tokenizer_mt = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-jap\")\n",
    "    model_mt = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-jap\")\n",
    "    tokenizer_mt.save_pretrained(MT_DIR)\n",
    "    model_mt.save_pretrained(MT_DIR)\n",
    "else:\n",
    "    print(\"âœ… Loading MT model from disk.\")\n",
    "    tokenizer_mt = MarianTokenizer.from_pretrained(MT_DIR)\n",
    "    model_mt = MarianMTModel.from_pretrained(MT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc23e0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”½ Loading TTS model from local cache...\n",
      " > tts_models/ja/kokoro/tacotron2-DDC is already downloaded.\n",
      " > vocoder_models/ja/kokoro/hifigan_v1 is already downloaded.\n",
      " > Using model: Tacotron2\n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:22050\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log10\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:1024\n",
      " | > power:1.5\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:True\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:50.0\n",
      " | > mel_fmax:7600.0\n",
      " | > pitch_fmin:0.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:1.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:True\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:C:\\Users\\ADMIN\\AppData\\Local\\tts\\tts_models--ja--kokoro--tacotron2-DDC\\scale_stats.npy\n",
      " | > base:10\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n",
      " > Model's reduction rate `r` is set to: 3\n",
      " > Vocoder Model: hifigan\n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:22050\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log10\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:1024\n",
      " | > power:1.5\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:True\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:50.0\n",
      " | > mel_fmax:7600.0\n",
      " | > pitch_fmin:0.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:1.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:True\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:C:\\Users\\ADMIN\\AppData\\Local\\tts\\vocoder_models--ja--kokoro--hifigan_v1\\scale_stats.npy\n",
      " | > base:10\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n",
      " > Generator Model: hifigan_generator\n",
      " > Discriminator Model: hifigan_discriminator\n",
      "Removing weight norm...\n"
     ]
    }
   ],
   "source": [
    "# ========== TTS: Set Local Cache and Load ==========\n",
    "print(\"ðŸ”½ Loading TTS model from local cache...\")\n",
    "os.environ[\"TTS_CACHE_PATH\"] = TTS_CACHE\n",
    "tts = TTS(model_name=\"tts_models/ja/kokoro/tacotron2-DDC\", progress_bar=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4b846b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ™ï¸ Recording for 5 seconds...\n",
      "âœ… Recording saved as recorded.wav\n"
     ]
    }
   ],
   "source": [
    "# ========== Step 1: Record from Mic ==========\n",
    "print(f\"ðŸŽ™ï¸ Recording for {DURATION} seconds...\")\n",
    "recording = sd.rec(int(DURATION * SAMPLE_RATE), samplerate=SAMPLE_RATE, channels=1, dtype='int16')\n",
    "sd.wait()\n",
    "write(AUDIO_FILENAME, SAMPLE_RATE, recording)\n",
    "print(\"âœ… Recording saved as\", AUDIO_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c5c60a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def spacy_capitalize(text):\n",
    "    doc = nlp(text)\n",
    "    sentences = []\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        capitalized = []\n",
    "        for i, token in enumerate(sent):\n",
    "            if i == 0 or token.ent_type_ or token.pos_ in ['PROPN']:\n",
    "                capitalized.append(token.text.capitalize())\n",
    "            else:\n",
    "                capitalized.append(token.text.lower())\n",
    "\n",
    "        sentence = ' '.join(capitalized)\n",
    "        # Clean spacing\n",
    "        sentence = sentence.replace(\" ,\", \",\").replace(\" .\", \".\").replace(\" '\", \"'\").replace(\" n't\", \"n't\")\n",
    "        sentences.append(sentence)\n",
    "\n",
    "    return ' '.join(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1bcafc3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my name is john\n",
      "ðŸ“ Restored: My name is John\n"
     ]
    }
   ],
   "source": [
    "# ========== Step 2: ASR - English Speech â†’ Text ==========\n",
    "waveform_np, sr = sf.read(AUDIO_FILENAME)\n",
    "waveform = torch.tensor(waveform_np).float().unsqueeze(0)\n",
    "\n",
    "if sr != 16000:\n",
    "    waveform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)(waveform)\n",
    "\n",
    "input_values = processor_asr(waveform.squeeze(), return_tensors=\"pt\", sampling_rate=16000).input_values\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model_asr(input_values).logits\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "# From ASR output\n",
    "english_text = processor_asr.batch_decode(predicted_ids)[0].lower().strip()\n",
    "print(english_text)\n",
    "english_text = spacy_capitalize(english_text)\n",
    "print(\"ðŸ“ Restored:\", english_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3376e027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒ Japanese Translation: ã‚ãŸã— ã® å ã¯ ãƒ¨ãƒãƒ ã¨ ã„ ã† .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inputs_mt = tokenizer_mt(english_text, return_tensors=\"pt\", padding=True)\n",
    "translated = model_mt.generate(**inputs_mt)\n",
    "japanese_text = tokenizer_mt.decode(translated[0], skip_special_tokens=True)\n",
    "print(\"ðŸŒ Japanese Translation:\", japanese_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e54386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text splitted to sentences.\n",
      "['ã‚ãŸã— ã® å ã¯ ãƒ¨ãƒãƒ ã¨ ã„ ã† .']\n",
      " > Processing time: 0.9275715351104736\n",
      " > Real-time factor: 0.4074942689907942\n",
      "ðŸ”Š Japanese speech saved to 'output_jp.wav'\n"
     ]
    }
   ],
   "source": [
    "# ========== Step 4: TTS - Synthesize Japanese ==========\n",
    "tts.tts_to_file(text=japanese_text, file_path=OUTPUT_WAV)\n",
    "print(f\"ðŸ”Š Japanese speech saved to '{OUTPUT_WAV}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
